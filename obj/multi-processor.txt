# -*- coding: utf-8 -*-
import os
import re
import urllib.request
from bs4 import BeautifulSoup
import pandas as pd
import time
import math
from multiprocessing import Pool
from multiprocessing import Process, Queue
import asyncio, aiohttp
import datetime

"""
def write_data():
    r = 0
    while True:
        n = yield r
        if n < 3:
            return
        print("Write 3 records into file")
        n = 0
        r = 1


def crawl_data(c):
    c.send(None)
    i = 0
    while i < 12:
        i += 1
        print("Crawl web page: " + str(i))
        if i % 3 == 0:
            r = c.send(i)
            #print("[WRITE return]: " + str(r))
    c.close()

start = time.clock()
c = write_data()
crawl_data(c)
end = time.clock()
span = end-start
print(span)
"""

class CrawlYP(Process):
    def __init__(self, i, sub_key_list, top_k): # i-process id, sub_key_list - sub set of keys to be crawled
        super(CrawlYP, self).__init__()
        self.i = i
        self.sub_key_list = sub_key_list
        self.top_k = top_k

    def run(self):
        filename = "D:\\business data\\multi-process_" + str(self.i) + ".txt"
        wf = open(filename, 'a+', encoding='utf-8')
        lines = ""
        write_count = 0
        for t in range(0, len(self.sub_key_list)):
            key = self.sub_key_list[t]
            tlist = self.getYPinfo(key, 1)
            count = 0
            key = tlist[0]
            res_list = tlist[1]
            if len(res_list) != 0:
                for item in res_list:
                    #name_cate = page_extract(item)
                    temp = ""
                    if count == 0:
                        temp = key + "\t" + item + "\t" + str(len(res_list)) + "\n"
                        lines += temp
                    else:
                        temp = "*\t" + item + "\t*\n"
                        lines += "*\t" + item + "\t*\n"
                    count += 1
                    #print(temp)
                print(key + '\t' + str(len(res_list)))
                write_count += len(res_list)
            else:
                #print(key + "\tNULL\tNULL\t0\n")
                print(key + "\t0")
                lines += key + "\tNULL\tNULL\t0\n"
                write_count += 1

            if write_count >= 3000:
                wf.write(lines)
                lines = ""
                write_count = 0
            #else:
                #time.sleep(2)
        if lines != "":
            wf.write(lines)
        wf.close()


    def getHtml(self, url):
        page = urllib.request.urlopen(url)
        html = page.read()
        return html

    def getYPinfo(self, key, m):
        ################ obtain the search page of YellowPage Canada with the top k results ################
        # res_list: keep the search result, in the form of 'name \t category'
        # return the list of candidate results
        num_per_page = 40
        res_list = []
        final_key = key.replace(" ", "%20")
        #print(final_key)

        status = 0

        num_of_spages = math.ceil(self.top_k / 40.0) # number of search result pages loaded

        request_num = 0
        res_num = 0
        for m in range(m, num_of_spages + 1):

            if m == num_of_spages and self.top_k % num_per_page != 0:
                request_num = self.top_k % num_per_page
            else:
                request_num = num_per_page

            #time.sleep(2)
            search_url = "https://www.yellowpages.ca/search/si/" + str(m) + "/" + final_key + "/canada"
            link_count = 0
            while link_count <= 10:
                try:
                    html_text = self.getHtml(search_url)
                    soup = BeautifulSoup(html_text, 'lxml', from_encoding="utf-8")
                    link_count = 0
                    break
                except Exception as e:
                    print("[PARAGRAPH" + str(self.i) + "][KEY: " + key + "]: html download error!")
                    print(repr(e))
                    link_count += 1
                    time.sleep(5 + link_count * 5)
                    continue
            """
            if link_count > 10:
                html_text = self.getHtml(search_url)
                soup = BeautifulSoup(html_text, 'lxml', from_encoding="utf-8")
                """

            try:

                search_content = soup.find('div', class_='resultList jsResultsList jsMLRContainer')
                res_count = 0
                child_count = 0
                #print(len(list(search_content.div.children)))

                for child in search_content.div.children:
                    if "div" in str(child) and child_count < request_num:
                        child_count += 1
                        try:
                            name_info = child.find('a', class_='listing__name--link listing__link jsListingName')
                            #print(name_info.string)
                        except Exception as e:
                            print("name extraction fail!!!")
                            print(child)
                            continue
                        try:
                            category_info = child.find('div', class_='listing__headings')
                            candidate_cate = category_info.strings
                            category = ""
                            for item in candidate_cate:
                                if item == ', ':
                                    category += "+"
                                else:
                                    category += item
                            #print(name_info.string + '\t\t' + category)
                            res_list.append(name_info.string + '\t' + category)
                            res_num += 1
                        except Exception as e:
                            #print(name_info.string + '\t\tNULL')
                            res_list.append(name_info.string + '\tNULL')
                            res_num += 1
                if child_count < request_num:
                    break
            except Exception as e:
                    # if the http request is refused by the server, then wait 30 seconds and continue to issue the request
                    if m == 1:
                        # no result
                        break
                    else:
                        # http request refused
                        print("[PARAGRAPH" + str(self.i) + "][KEY: " + key + "]:search content extraction fail: KEY " + key + ", PAGE: " + str(m))
                        time.sleep(10)
                        res_list += self.getYPinfo(m)
        #print(res_num)
        final_list = []
        final_list.append(key)
        final_list.append(res_list)
        return final_list




def main():
    pd.set_option('display.max_columns', None)
    data = pd.read_pickle("C:\\Users\\Liang\\Downloads\\v\\vancity.pkl")
    name_list = data["ENT_ORG_NAME"][:-1]
    print(len(name_list))

    #q = Queue()
    Process_list = []
    top_k = 500
    for x in range(2, 10):
        step_len = math.ceil(len(name_list)/10)
        print(str(step_len))
        if x != 9:
            sub_key_list = []
            for i in range(x * step_len, x * step_len + step_len):
                sub_key_list.append(name_list[i])
            #sub_key_list = name_list[x * step_len: x * step_len + step_len]
            p = CrawlYP(x, sub_key_list, top_k)
        else:
            sub_key_list = []
            for i in range(x * step_len, len(name_list)):
                sub_key_list.append(name_list[i])
            #sub_key_list = name_list[x * step_len:]
            p = CrawlYP(x, sub_key_list, top_k)
        p.start()
        Process_list.append(p)
    """
    for i in Process_list:
        i.join()
        """
    """
    start = time.clock()
    lines = ""
    write_count = 0
    while not q.empty():
        list = q.get()
        count = 0
        key = list[0]
        res_list = list[1]
        if len(res_list) != 0:
            for item in res_list:
                #name_cate = page_extract(item)
                temp = ""
                if count == 0:
                    temp = key + "\t" + item + "\t" + str(len(res_list)) + "\n"
                    lines += temp
                else:
                    temp = "*\t" + item + "\t*\n"
                    lines += "*\t" + item + "\t*\n"
                count += 1
                #print(temp)
            print(key + '\t' + str(len(res_list)))
            write_count += len(res_list)
        else:
            #print(key + "\tNULL\tNULL\t0\n")
            print(key + "\t0")
            lines += key + "\tNULL\tNULL\t0\n"
            write_count += 1

        if write_count >= 3000:
            wf.write(lines)
            lines = ""
            write_count = 0
        else:
            time.sleep(2)
    if lines != "":
        wf.write(lines)
    wf.close()
    span = time.clock() - start
    print("Write File: " + str(span))
    """


if __name__=="__main__":
    main()